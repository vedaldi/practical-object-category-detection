{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object category detection practical\n",
    "\n",
    "This is an [Oxford Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg) computer vision practical, authored by [Andrea Vedaldi](http://www.robots.ox.ac.uk/~vedaldi/) and Andrew Zisserman (Release 2018a).\n",
    "\n",
    "![doc/cover][1]\n",
    "\n",
    "The goal of *object category detection* is to identify and localize objects of a given type in an image. Examples applications include detecting pedestrian, cars, or traffic signs in street scenes, objects of interest such as tools or animals in web images, or particular features in medical image. Given a target class, such as *people*, a *detector* receives as input an image and produces as output zero, one, or more bounding boxes around each occurrence of the object class in the image. The key challenge is that the detector needs to find objects regardless of their location and scale in the image, as well as pose and other variation factors, such as clothing, illumination, occlusions, etc.\n",
    "\n",
    "This practical explores basic techniques in visual object detection, focusing on  *image based models*. The appearance of image patches containing objects is learned using statistical analysis. Then, in order to detect objects in an image, the statistical model is applied to image windows extracted at all possible scales and locations, in order to identify which ones, if any, contain the object.\n",
    "\n",
    "In more detail, the practical explores the following topics: (i) using HOG features to describe image regions, (ii) building a HOG-based sliding-window detector to localize objects in images; (iii) working with multiple scales and multiple object occurrences; (iv) using a linear support vector machine to learn the appearance of objects; (v) evaluating an object detector in term of average precision; (vi) learning an object detector using hard negative mining.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bw}{\\mathbf{w}}\n",
    "\\newcommand{\\bp}{\\mathbf{p}}\n",
    "\\newcommand{\\cP}{\\mathcal{P}}\n",
    "\\newcommand{\\cN}{\\mathcal{N}}\n",
    "\\newcommand{\\vv}{\\operatorname{vec}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## Part 1: Detection fundamentals\n",
    "\n",
    "As running example, we consider the problem of street sign detection, using the data from the [German Traffic Sign Detection Benchmark](http://benchmark.ini.rub.de/?section=gtsdb&subsection=news). This data consists of a number of example traffic images, as well as a number of larger test images containing one or more traffic signs at different sizes and locations. It also comes with *ground truth* annotation, i.e. with specified bounding boxes and sign labels for each sign occurrence, which is required to evaluate the quality of the detector.\n",
    "\n",
    "In this part we will build a basic sliding-window object detector based on HOG features. Follow the steps below.\n",
    "\n",
    "### Step 1.0: Loading the training data\n",
    "\n",
    "First, we will load a Python data structure containing the data for the practical. To this end, we use the `lab.load_data` function of the `lab` package and store the result in a dictionary `imdb`. This dictionary has fields:\n",
    "\n",
    "* `imdb['train']['images']`: a list of train image names.\n",
    "* `imdb['train']['boxes']`: a $N\\times 4$ tensor of object bounding boxes, in the form $[x_\\text{min},y_\\text{min},x_\\text{max},y_\\text{max}]$.\n",
    "* `imdb['train']['box_images']`: for each bounding box, the name of the image containing it.\n",
    "* `imdb['train']['box_labels']`: for each bounding box, the object label.\n",
    "* `imdb['train']['box_patches']`: a $N \\times 3 64 \\times 64$ tensor of image patches, one for each training object. Patches are in RGB format.\n",
    "\n",
    "An analogous set of fields `imdb['val']` are provided for the validation data. Familiarise yourself with the contents of these variables.\n",
    "\n",
    "> **Task:** Run the following code to load the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import lab\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "imdb = lab.load_data('all') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** why is there a `imdb['train']['images']` and a `imdb['train']['box_images']` fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Visualize the training image patches\n",
    "\n",
    "Next, we use the functon `lab.imarraysc` to display the image patches stored in `imdb['train']['box_patches']`. The `lab.imarraysc` takes as input a $N\\times3\\times H\\times W$ tensor of $N$ RGB images of size $H\\times W$. We also plot the mean of these patches using the function `imsc`, which instead takes as input a single $3\\times H\\times W$ image.\n",
    "\n",
    "> **Task:** Run the following code to visualize the training image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training patches\n",
    "plt.figure(1, figsize=(12,12))\n",
    "lab.imarraysc(imdb['train']['box_patches'])\n",
    "plt.title('Training patches')\n",
    "\n",
    "# Plot the average patch\n",
    "plt.figure(2,figsize=(8,8))\n",
    "lab.imsc(imdb['train']['box_patches'].mean(0))\n",
    "plt.title('Training patches mean') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select now the part of the code related to section 1.1 and execute it. This will create an image visualizing both the complete list of object training examples and their average.\n",
    "\n",
    "> **Question:** what can you deduce about the object variability from the average image?\n",
    "\n",
    "> **Question:** most boxes extend slightly around the object extent. Why do you think this may be valuable in learning a detector?\n",
    "\n",
    "### Step 1.2: Extract HOG features from the training patches\n",
    "\n",
    "Object detectors usually work on top of a layer of low-level features. We will use a *Convolutional Neural Network* to extract such features. However, instead of learning it from data as it would be customary done, we take a shortcut and create an *handcrafted* network that computes the HOG (*Histogram of Oriented Gradients*) features. In this manner, we can still make efficient use of the underlying PyTorch library.\n",
    "\n",
    "In order to learn a model of the object, we start by extracting features from the image patches corresponding to the available training examples. HOG is computed by the `lab.HOGNet()` neural network. This network takes as input a grayscale or RGB image (or a batch of $N$ images), represented as a PyTorch tensor $N \\times C \\times W \\times H$ array where $C$ is either 1 or 3. The output is a $N \\times 27 \\times W/\\text{cell-size} \\times H/\\text{cell-size}$ dimensional array, where `cell_size` is 8 by default.\n",
    "\n",
    "> **Task:** Run the following code to extract the HOG representation of the training patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HOG extractor neural network\n",
    "hog_extractor = lab.HOGNet()\n",
    "\n",
    "# Get the HOG representation of the trianing patches.\n",
    "hog_train = hog_extractor(imdb['train']['box_patches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1.3\"></a>\n",
    "\n",
    "### Step 1.3: Learn a simple HOG template model \n",
    "\n",
    "A very basic object model can be obtained by averaging the features of the example objects, as follows.\n",
    "\n",
    "> **Task:** Run the following colde to train a basic model for a target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image class\n",
    "sign_type = 1\n",
    "\n",
    "# Select the corresponding training patches\n",
    "train = imdb['train']['box_labels'] == sign_type\n",
    "print(f\"There are {sum(train)} trainig patches for the sign type {sign_type}\")\n",
    "\n",
    "# Average the HOG of the corresponding training images\n",
    "w = torch.mean(hog_train[train, :, :, :], 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be visualized by *rendering* `w` as if it was a HOG feature array. This can be done using the `hog_extractor.to_image` method. This method takes a $N\\times 27 \\times H\\times W$ HOG tensor and return a corresponding $N\\times 1\\times H_r\\times W_r$ tensor of rendered HOG images.\n",
    "\n",
    "> **Task:** Run the following code to visualize the model `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the HOG descriptor\n",
    "wim = hog_extractor.to_image(w)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6,6))\n",
    "lab.imsc(wim[0]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time to study this plot and make sure you understand what is visualized.\n",
    "\n",
    "> **Question:** Can you make sense of the resulting plot?\n",
    "\n",
    "### Step 1.4: Apply the model to a test image\n",
    "\n",
    "The model is matched to a test image by: (i) extracting the HOG features of the image and (ii) convolving the model over the resulting feature map. This can be conveniently achieved by using PyTorch convlutional operators.\n",
    "\n",
    "> **Task:** Run the following code to apply the learned model to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Wrap the model w in a convolutional layer\n",
    "model = nn.Conv2d(27, 1, w.shape[2:], bias=False)\n",
    "model.weight.data = w\n",
    "print(model)\n",
    "\n",
    "# Extract the HOG representation of a test image\n",
    "im = lab.imread('data/signs-sample-image.jpg')\n",
    "hog = hog_extractor(im)\n",
    "\n",
    "# Apply the model\n",
    "scores = model(hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines read a sample image and conver it to single format. The third line computes the HOG features of the image using the `vl_hog` seen above. The fourth line convolves the HOG map `hog` with the model `w`. It uses the function `vl_nnconv`[^nn] and returns a `scores` map.\n",
    "\n",
    "> **Tasks:** \n",
    "> 1. Work out the dimension of the `scores` arrays. Then, check your result with the dimension of the array computed by MATLAB.\n",
    "> 2. Visualize the image `im` and the `scores` array using the provided example code below. Does the result match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image\n",
    "plt.figure(1, figsize=(8,8))\n",
    "lab.imsc(im[0])\n",
    "plt.title('Input image')\n",
    "\n",
    "# Visualize its HOG representation\n",
    "plt.figure(2, figsize=(8,8))\n",
    "lab.imsc(hog_extractor.to_image(hog)[0])\n",
    "plt.title('HOG representation')\n",
    "\n",
    "# Visualize the scores as a heatmap\n",
    "plt.figure(3, figsize=(8,8))\n",
    "lab.imsc(scores[0])\n",
    "plt.title('Scores') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Extract the top detection\n",
    "\n",
    "Now that the model has been applied to the image, we have a response map `scores`. To extract a detection from this, we (i) find which image boxes correspond to each entry in the `scores` tensor and (ii) find the box that has mximum score.\n",
    "\n",
    "The first step is done by the `boxes_for_scores` function, which takes as input a $1\\times H\\times W$ score map and returns as output a $4\\times H\\times W$ tensor `boxes` where `boxes[:,v_,u_]` is a vector $[u_0,v_0,u_1,v_1]$ representing the bounding box in image space corresponding to score value `scores[1,v_,u_]`. Here $(u_0,v_0)$ is the upper-left corner of the box and $(u_1,v_1)$ the lower-right.\n",
    "\n",
    "The second step is done by rearranging `boxes` as a $N \\times 4$ tensor representing a list of $N=HW$ boxes, one per row, rearranging `scores` as a `N` vector, finding the maximum element of the latter, and use the corresponding index to pick a box from `boxes`.\n",
    "\n",
    "> **Question:** Inspect the code below. Why `cell_size` is involved in the calculations performed by `boxes_for_scores`?\n",
    "\n",
    "> **Task.** Run the following code to find the maximally-scored box and plot a corresponding bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_for_scores(model, scores, cell_size=8):\n",
    "    mh = model.kernel_size[0]\n",
    "    mw = model.kernel_size[1]\n",
    "    with torch.no_grad():\n",
    "        h = scores.shape[1]\n",
    "        w = scores.shape[2]\n",
    "        v0 = torch.arange(h).reshape(1,-1,1)\n",
    "        u0 = torch.arange(w).reshape(1,1,-1)\n",
    "        v1 = v0 + mh\n",
    "        u1 = u0 + mw\n",
    "        boxes = torch.cat([(x * cell_size).expand(1,h,w) for x in [u0,v0,u1,v1]], 0)\n",
    "    return boxes\n",
    "\n",
    "# Get the box coordinates for each score map location as a 4 x H x W tensor where\n",
    "# score[0] is a 1 x H x W tensor.\n",
    "boxes = boxes_for_scores(model, scores[0])\n",
    "\n",
    "# Convert `boxes` into a N x 4 list.\n",
    "boxes = boxes.reshape(4,-1).permute(1,0)\n",
    "\n",
    "# Do the same for `scores` and pick the maximum.\n",
    "best, best_index = torch.max(scores[0].reshape(-1), 0)\n",
    "box = boxes[best_index]\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure(1, figsize=(10,10))\n",
    "lab.imsc(im[0])\n",
    "lab.plot_box(box)\n",
    "plt.title('Detected object');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The maximum is found in two steps, by first maximizing scores along dimension 2 (height) and then dimension 1 (width), obtaining indices `u` and `v`.\n",
    "\n",
    "`u` and `v` are in units of HOG cells. We convert this into pixel coordinates by multiplying by the HOG `cell_size`, which is also the downsampling factor applied by the `HOGNet` feature extractor.\n",
    "\n",
    "The size of the model template in number of HOG cell can be found as the size of the kernel in `model` (note that the kernel is square in this case, so PyTorch only saves one dimension). This is then converted in pixels by multipling by `cell_size` as before.\n",
    "\n",
    "In this manner, we can deduce the coordiantes of the upper-left and bottom-right conrenrs of the object bounding box and plot it.\n",
    "\n",
    "**Note:** the bounding box encloses exactly all the pixel of the HOG template. In MATLAB, pixel centers have integer coordinates and pixel borders are at a distance $\\pm1/2$.\n",
    "\n",
    "> **Question:** Use the example code to plot the image and overlay the bounding box of the detected object. Did it work as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple scales and learning with an SVM {#part2}\n",
    "\n",
    "In this second part, we will: (i) extend the detector to search objects at multiple scales and (ii) learn a better model using a support vector machine. Let's start by loading a subset of the data targeting a particular class of road signs.\n",
    "\n",
    "> **Task:** Run the following code to reload the data structure keeping only the \"mandatory\" signs.\n",
    "\n",
    "The `mandatory` class is simply the union of all mandatory traffic signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = lab.load_data(meta_class='mandatory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2.1\"></a>\n",
    "\n",
    "### Step 2.1: Multi-scale detection\n",
    "\n",
    "Objects exist in images at sizes different from one of the learned template. In order to find objects of all sizes, we scale the image up and down and search for the object over and over again.\n",
    "\n",
    "The set of searched scales $s_0,s_1,\\dots$ is defined logarithmically as $s_i = 2^{\\frac{i}{S} - o}$ where $S$ is the number of octave subdivisions and $o$ the minimum octave.\n",
    "\n",
    "> **Task**: Run the following code to define the set of tested octaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scale space configuraiton\n",
    "min_octave = -1\n",
    "max_octave = 3\n",
    "num_octave_subdivisions = 3\n",
    "scales = 2**np.linspace(min_octave, max_octave, num_octave_subdivisions * (max_octave - min_octave + 1))\n",
    "\n",
    "# Print the scale values\n",
    "print('Scales: ', ', '.join([f\"{x:.2f}\" for x in scales]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the model `w`, as determined in Part I, we use the function `detect_at_multiple_scales` in order to search for the object at multiple scales.\n",
    "\n",
    "> **Task:** Run the following code to detected boxes on scaled versions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def detect_at_multiple_scales(w, scales, pil_image):\n",
    "    # Wrap parameters w in a convolutional layer\n",
    "    model = nn.Conv2d(27, 1, w.shape[2:], bias=False)\n",
    "    model.weight.data = w\n",
    "    cell_size = hog_extractor.cell_size\n",
    "\n",
    "    # Search for strong responses across different scales\n",
    "    all_scores = []\n",
    "    all_boxes = []\n",
    "    for scale in scales:\n",
    "        # Scale the input image\n",
    "        size = [int(round(x/scale)) for x in pil_image.size]\n",
    "        scaled_image = pil_image.resize(size)\n",
    "\n",
    "        # Skip if the scaled image is smaller than 2 x 2 HOG cells\n",
    "        if scaled_image.size[0] < 2*cell_size or scaled_image.size[1] < 2*cell_size:\n",
    "            continue\n",
    "\n",
    "        # Extract its HOG representation\n",
    "        hog = hog_extractor(lab.pil_to_torch(scaled_image))\n",
    "\n",
    "        # Skip if the HOG representation is smaller than the model\n",
    "        if hog.shape[2] < w.shape[2] or hog.shape[3] < w.shape[3]:\n",
    "            continue\n",
    "            \n",
    "        # Apply the model convolutionally\n",
    "        scores = model(hog)\n",
    "        \n",
    "        # Get the boxes\n",
    "        boxes = boxes_for_scores(model, scores[0])\n",
    "        \n",
    "        # Reshape into lists\n",
    "        boxes = boxes.reshape(4,-1).permute(1,0)\n",
    "        scores = scores.reshape(-1)\n",
    "        \n",
    "        # Store for later\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    # Concatenate    \n",
    "    return torch.cat(all_boxes, 0), torch.cat(all_scores, 0)\n",
    "    \n",
    "import PIL\n",
    "\n",
    "for t, s in enumerate([1, .75, .5]):\n",
    "    # Load a target image in PIL format\n",
    "    image = Image.open('data/signs-sample-image.jpg')\n",
    "    \n",
    "    # Resize\n",
    "    image = image.resize([int(s*x) for x in image.size], PIL.Image.LANCZOS)\n",
    "\n",
    "    # Get all boxes and scores\n",
    "    boxes, scores = detect_at_multiple_scales(w, scales, image)\n",
    "\n",
    "    # Pick the best box\n",
    "    best, best_index = torch.max(scores, 0)\n",
    "    box = boxes[best_index]\n",
    "\n",
    "    # Plot the results.\n",
    "    plt.figure(t, figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    lab.plot_box(box)\n",
    "    plt.title(f'Detected object ({best:.3})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function generates a figure as it runs, so prepare a new figure before running it using the `figure` command if you do not want your current figure to be deleted.\n",
    "\n",
    "> **Question:** Open and study the `detect_at_multiple_scales` function. Convince yourself that it is the same code as before, but operated after rescaling the image a number of times. \n",
    "\n",
    "> **Question:** Visualize the resulting detection using the supplied example code. Did it work? If not, can you make sense of the errors?\n",
    "\n",
    "> **Question:** Look at the array of `scores` maps generated by `detectAtMultipleScales` using the example code. Do they make sense? Is there anything wrong?\n",
    "\n",
    "### Step 2.2: Collect positive and negative training data\n",
    "\n",
    "The model learned so far is too weak to work well. It is now time to use an SVM to learn a better one. In order to do so, we need to prepare suitable data. We already have positive examples (features extracted from object patches):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect positive training data\n",
    "pos = hog_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect negative examples (features extracted from non-object patches), we loop through a number of training images and sample patches uniformly. For speed, we only scan a subset of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "num_training_images = 5\n",
    "for image_path in imdb['train']['images'][:num_training_images]:\n",
    "    print(f\"Processing image {image_path}\")\n",
    "    image = Image.open(image_path)    \n",
    "    hog = hog_extractor(lab.pil_to_torch(image))    \n",
    "    patches = nn.Unfold(w.shape[1:], stride=5)(hog)\n",
    "    n = patches.shape[1]\n",
    "    patches = patches[:, :, :-1:n//100].permute(0,2,1)\n",
    "    patches = patches.reshape(patches.shape[1], *w.shape)\n",
    "    neg.append(patches)\n",
    "    \n",
    "neg = torch.cat(neg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Identify the code that extract these patches in `example2.m` and make sure you understand it.\n",
    "\n",
    "> **Question:** How many negative examples are we collecting?\n",
    "\n",
    "### Step 2.3: Learn a model with an SVM\n",
    "\n",
    "Now that we have the data, we can learn an SVM model. To this end we will use the `lab.svm_scda()` function. This function requires the data to be in a $D \\times N$ matrix, where $D$ are the feature dimensions and $N$ the number of training points. We also need a vector of binary labels, +1 for positive points and -1 for negative ones:\n",
    "\n",
    "This is done by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the data into a matrix with one datum per row\n",
    "x = torch.cat((pos, neg), 0).reshape(len(pos) + len(neg), -1)\n",
    "\n",
    "# Obtain the corresponding label vector\n",
    "y = torch.cat((torch.ones(len(pos)), -torch.ones(len(neg))), 0)\n",
    "\n",
    "print(f\"The shape of x is {list(x.shape)}\")\n",
    "print(f\"The shape of y is {list(y.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to set the parameter $\\lambda$ of the SVM solver. For reasons that will become clearer later, we use instead the equivalent $C$ parameter:Learning the SVM is then a one-liner:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "lam = 1 / (C * (len(pos) + len(neg)))\n",
    "\n",
    "w, b = lab.svm_sdca(x,y,lam=lam)\n",
    "w = w.reshape(pos[0].shape)\n",
    "\n",
    "# Plot the learned model\n",
    "wim = hog_extractor.to_image(w[None,:])\n",
    "plt.figure(figsize=(6,6))\n",
    "lab.imsc(wim[0]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Visualize the learned model `w` using the supplied code. Does it differ from the naive model learned before? How?\n",
    "\n",
    "### Step 2.4: Evaluate the learned model\n",
    "\n",
    "Use the `detectAtMultipleScales` seen above to evaluate the new SVM-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, s in enumerate([1, .75, .5]):\n",
    "    # Load a target image in PIL format\n",
    "    image = Image.open('data/signs-sample-image.jpg')\n",
    "    \n",
    "    # Resize\n",
    "    image = image.resize([int(s*x) for x in image.size], PIL.Image.LANCZOS)\n",
    "\n",
    "    # Get all boxes and scores\n",
    "    boxes, scores = detect_at_multiple_scales(w, scales, image)\n",
    "\n",
    "    # Pick the best box\n",
    "    best, best_index = torch.max(scores, 0)\n",
    "    box = boxes[best_index]\n",
    "\n",
    "    # Plot the results.\n",
    "    plt.figure(t, figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    lab.plot_box(box)\n",
    "    plt.title('Detected object');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Does the learned model perform better than the naive average?\n",
    "\n",
    "> **Task:** Try different images. Does this detector work all the times? If not, what types of mistakes do you see? Are these mistakes reasonable?\n",
    "\n",
    "## Part 3: Multiple objects and evaluation {#part3}\n",
    "\n",
    "### Step 3.1: Multiple detections\n",
    "\n",
    "Detecting at multiple scales is insufficient: we must also allow for more than one object occurrence in the image. In order to to so, the package include a suitalbe `detect` function. This function is similar to `detectAtMultipleScales`, but it returns the top 1000 detector responses rather than just the top one:\n",
    "```matlab\n",
    "% Compute detections\n",
    "[detections, scores] = detect(im, w, hogCellSize, scales) ;\n",
    "```\n",
    "\n",
    "> **Task:** Open and study `detect.m`. Make sure that you understand how it works.\n",
    "\n",
    "> **Question:** Why do we want to return so many responses? In practice, it is unlikely that more than a handful of object occurrences may be contained in any given image...\n",
    "\n",
    "A single object occurrence generates multiple detector responses at nearby image locations and scales. In order to eliminate these redundant detections, we use a *non-maximum suppression* algorithm. This is implemented by the `boxsuppress.m` MATLAB m-file. The algorithm is simple: start from the highest-scoring detection, then remove any other detection whose overlap[^overlap] is greater than a threshold. The function returns a boolean vector `keep` of detections to preserve:\n",
    "\n",
    "```matlab\n",
    "% Non-maximum suppression\n",
    "keep = boxsuppress(detections, scores, 0.25) ;\n",
    "\n",
    "detections = detections(:, keep) ;\n",
    "scores = scores(keep) ;\n",
    "```\n",
    "\n",
    "For efficiency, after non-maximum suppression we keep just ten responses (as we do not expect more than a few objects in any image):\n",
    "```matlab\n",
    "% Further keep only top detections\n",
    "detections = detections(:, 1:10) ;\n",
    "scores = scores(1:10) ;\n",
    "```\n",
    "\n",
    "### Step 3.2: Detector evaluation\n",
    "\n",
    "We are now going to look at properly evaluating our detector. We use the [PASCAL VOC criterion](http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/devkit_doc.pdf), computing *Average Precision (AP)*. Consider a test image containing a number of ground truth object occurrences $(g_1,\\dots,g_m)$ and a list $(b_1,s_1),\\dots,(b_n,s_n)$ of candidate detections $b_i$ with score $s_i$. The following algorithm converts this data into a list of labels and scores $(s_i,y_i)$ that can be used to compute a precision-recall curve, for example using VLFeat `vl_pr` function. The algorithm, implemented by `evalDetections.m`, is as follows:\n",
    "\n",
    "1. Assign each candidate detection $(b_i,s_i)$ a true or false label $y_i \\in \\{+1,-1\\}$. To do so:\n",
    "    1. The candidate detections $(b_i,s_i)$ are sorted by decreasing score $s_i$.\n",
    "    2. For each candidate detection in order:\n",
    "        a. If there is a matching ground truth detection $g_j$ ($\\operatorname{overlap}(b_i,g_j)$ larger than 50%), the candidate detection is considered positive ($y_i=+1$). Furthermore, the ground truth detection is *removed from the list* and not considered further.\n",
    "        b. Otherwise, the candidate detection is negative ($y_i=-1$).\n",
    "2. Add each ground truth object $g_i$ that is still unassigned to the list of candidates as pair $(g_j, -\\infty)$ with label $y_j=+1$.\n",
    "\n",
    "The overlap metric used to compare a candidate detection to a ground truth bounding box is defined as the *ratio of the area of the intersection over the area of the union* of the two bounding boxes:\n",
    "$$\n",
    "\\operatorname{overlap}(A,B) = \\frac{|A\\cap B|}{|A \\cup B|}.\n",
    "$$\n",
    "\n",
    "> **Questions:**\n",
    "\n",
    "> * Why are ground truth detections removed after being matched?\n",
    "> * What happens if an object is detected twice?\n",
    "> * Can you explain why unassigned ground-truth objects are added to the list of candidates with $-\\infty$ score?\n",
    "\n",
    "In order to apply this algorithm, we first need to find the ground truth bounding boxes in the test image:\n",
    "```matlab\n",
    "% Find all the objects in the target image\n",
    "s = find(strcmp(testImages{1}, testBoxImages)) ;\n",
    "gtBoxes = testBoxes(:, s) ;\n",
    "```\n",
    "\n",
    "Then `evalDetections` can be used:\n",
    "```matlab\n",
    "% No example is considered difficult\n",
    "gtDifficult = false(1, numel(s)) ;\n",
    "\n",
    "% PASCAL-like evaluation\n",
    "matches = evalDetections(...\n",
    "  gtBoxes, gtDifficult, ...\n",
    "  detections, scores) ;\n",
    "```\n",
    "The `gtDifficult` flags can be used to mark some ground truth object occurrence as *difficult* and hence ignored in the evaluation. This is used in the PASCAL VOC challenge, but not here (i.e. no object occurrence is considered difficult).\n",
    "\n",
    "`evalDetections` returns a `matches` structure with several fields. We focus here on `matches.detBoxFlags`: this contains a +1 for each detection that was found to be correct and -1 otherwise. We use this to visualize the detection errors:\n",
    "```matlab\n",
    "% Visualization\n",
    "figure(1) ; clf ;\n",
    "imagesc(im) ; axis equal ; hold on ;\n",
    "vl_plotbox(detections(:, matches.detBoxFlags==+1), 'g', 'linewidth', 2) ;\n",
    "vl_plotbox(detections(:, matches.detBoxFlags==-1), 'r', 'linewidth', 2) ;\n",
    "vl_plotbox(gtBoxes, 'b', 'linewidth', 1) ;\n",
    "axis off ;\n",
    "```\n",
    "\n",
    "> **Task:** Use the supplied example code to evaluate the detector on one image. Look carefully at the output and convince yourself that it makes sense.\n",
    "\n",
    "Now Plot the PR curve:\n",
    "```matlab\n",
    "figure(2) ; clf ;\n",
    "vl_pr(matches.labels, matches.scores) ;\n",
    "```\n",
    "\n",
    "> **Question:** There are a large number of errors in each image. Should you worry?  In what manner is the PR curve affected? How would you eliminate the vast majority of those in a practice?\n",
    "\n",
    "### Step 3.3: Evaluation on multiple images\n",
    "\n",
    "Evaluation is typically done on multiple images rather than just one. This is implemented by the `evalModel.m` m-file.\n",
    "\n",
    "> **Task:** Open `evalModel.m` and make sure you understand the main steps of the evaluation procedure.\n",
    "\n",
    "Use the supplied example code to run the evaluation on the entiere test set:\n",
    "```matlab\n",
    "matches = evaluateModel(testImages, testBoxes, testBoxImages, ...\n",
    "  w, hogCellSize, scales) ;\n",
    "```\n",
    "\n",
    "**Note:** The function processes an image per time, visualizing the results as it progresses. The PR curve is the result of the *accumulation* of the detections obtained thus far.\n",
    "\n",
    "> **Task:** Open the `evaluateModel.m` file in MATLAB and add a breakpoint right at the end of the for loop. Now run the evaluation code again and look at each image individually (use `dbcont` to go to the next image). Check out the correct and incorrect matches in each image and their ranking and the effect of this in the cumulative precision-recall curve.\n",
    "\n",
    "## Part 4: Hard negative mining {#part4}\n",
    "\n",
    "This part explores more advanced learning methods. So far, the SVM has been learned using a small and randomly sampled number of negative examples. However, in principle, every single patch that does not contain the object can be considered as a negative sample. These are of course too many to be used in practice; unfortunately, random sampling is ineffective as the most interesting (confusing) negative samples are a very small and special subset of all the possible ones.\n",
    "\n",
    "*Hard negative mining* is a simple technique that allows finding a small set of key negative examples. The idea is simple: we start by training a model without any negatives at all (in this case the solver learns a 1-class SVM), and then we alternate between evaluating the model on the training data to find erroneous responses and adding the corresponding examples to the training set.\n",
    "\n",
    "### Step 4.1: Train with hard negative mining {#stage4.1}\n",
    "\n",
    "Use the supplied code in `example4.m` to run hard negative mining. The code repeats SVM training, as seen above, a number of times, progressively increasing the size of the `neg` array containing the negative samples. This is updated using the output of:\n",
    "\n",
    "```matlab\n",
    " [matches, moreNeg] = ...\n",
    "    evaluateModel(...\n",
    "    vl_colsubset(trainImages', schedule(t), 'beginning'), ...\n",
    "    trainBoxes, trainBoxImages, ...\n",
    "    w, hogCellSize, scales) ;\n",
    "```\n",
    "\n",
    "Here `moreNeg` contains the HOG features of the top (highest scoring and hence most confusing) image patches in the supplied training images.\n",
    "\n",
    "> **Task:** Examine `evaluateModel.m` again to understand how hard negatives are extracted.\n",
    "\n",
    "> **Question:** What is the purpose of the construct `vl_colsubset(trainImages', schedule(t), 'beginning')`? Why do you think we visit more negative images in later iterations?\n",
    "\n",
    "The next step is to fuse the new negative set with the old one:\n",
    "```matlab\n",
    "% Add negatives\n",
    "neg = cat(4, neg, moreNeg) ;\n",
    "```\n",
    "\n",
    "Note that hard negative mining could select the same negatives at different iterations; the following code squashes these duplicates:\n",
    "```matlab\n",
    "% Remove negative duplicates\n",
    "z = reshape(neg, [], size(neg,4)) ;\n",
    "[~,keep] = unique(z','stable','rows') ;\n",
    "neg = neg(:,:,:,keep) ;\n",
    "```\n",
    "\n",
    "### Step 4.2: Evaluate the model on the test data\n",
    "\n",
    "Once hard negative mining and training are done, we are ready to evaluate the model on the *test* data (note that the model is evaluated on the *training* data for mining). As before:\n",
    "```matlab\n",
    "evaluateModel(...\n",
    "    testImages, testBoxes, testBoxImages, ...\n",
    "    w, hogCellSize, scales) ;\n",
    "```\n",
    "\n",
    "## Part 5: Train your own object detector\n",
    "\n",
    "**Skip on fast track**\n",
    "\n",
    "In this last part, you will learn your own object detector. To this end, open and look at `exercise5.m`. You will need to prepare the following data:\n",
    "\n",
    "### Step 5.1: Preparing the training data\n",
    "\n",
    "* A folder `data/myPositives` containing files `image1.jpeg`, `image2.jpeg`, ..., each containing a single cropped occurence of the target object. These crops can be of any size, but should be roughly square.\n",
    "* A folder `data/myNegatives` containing images `image1.jpeg`, `image2.jpeg`, ..., that *do not* contain the target object at all.\n",
    "* A test image `data/myTestImage.jpeg` containing the target object. This should not be one of the training images.\n",
    "\n",
    "Run the code in `example5.m` to check that your training data looks right.\n",
    "\n",
    "> **Task:** Understand the limitations of this simple detector and choose a target object that has a good chance of being learnable. \n",
    "\n",
    "**Hint:** Note in particular that object instances must be similar and roughly aligned. If your object is not symmetric, consider choosing instances that face a particular direction (e.g. left-facing horse head).\n",
    "\n",
    "### Step 5.2: Learn the model\n",
    "\n",
    "Use the code supplied in `example5.m` to learn an SVM model for your object using hard negative mining as in [Stage 4.1](#stage4.1).\n",
    "\n",
    "### Step 5.3: Test the model\n",
    "\n",
    "Use the code supplied in `example5.m` to evaluate the SVM model on a test image and visualize the result as in [Stage 2.1](#stage2.1).\n",
    "\n",
    "> **Task:** Make sure you get sensible results. Go back to step 5.1 if needed and adjust your data.\n",
    "\n",
    "**Hint:** For debugging purposes, try using one of your training images as test. Does it work at least in this case?\n",
    "\n",
    "### Step 5.4: Detecting symmetric objects with multiple aspects\n",
    "\n",
    "The basic detectors you have learned so far are *not* invariant to effects such as object deformations, out-of-plane rotations, and partial occlusions that affect most natural objects. Handling these effects requires additional sophistications, including using deformable templates, and a mixture of multiple templates.\n",
    "\n",
    "In particular, many objects in nature are symmetric and, as such, their images appear flipped when the objects are seen from the left or the right direction (consider for example a face). This can be handled by a pair of symmetric HOG templates. In this part we will explore this option.\n",
    "\n",
    "> **Task:** Using the procedure above, train a HOG template `w` for a symmetric object facing in one specific direction. For example, train a left-facing horse head detector.\n",
    "\n",
    "> **Task:** Collect test images containing the object facing in both directions. Run your detector and convince yourself that it works well only for the direction it was trained for.\n",
    "\n",
    "HOG features have a well defined structure that makes it possible to predict how the features transform when the underlying image is flipped. The transformation is in fact a simple *permutation* of the HOG elements. For a given spatial cell, HOG has 31 dimensions. The following code permutes the dimension to flip the cell around the vertical axis:\n",
    "\n",
    "    perm = vl_hog('permutation') ;\n",
    "    hog_flipped = hog(perm) ;\n",
    "\n",
    "Note that this permutation applies to a *single* HOG cell. However, the template is a $H \\times W \\times 31$ dimensional array of HOG cells.\n",
    "\n",
    "> **Task:** Given a `hog` array of dimension $H \\times W \\times 31$, write MATLAB code to obtain the flipped feature array `hog_flipped`.\n",
    "\n",
    "**Hint:** Recall that the first dimension spans the vertical axis, the second dimension the horizontal axis, and the third dimension feature channels. `perm` should be applied to the last dimension. Do you need to permute anything else?\n",
    "\n",
    "Now let us apply flipping to the model trained earlier:\n",
    "\n",
    "> **Task:** Let `w` be the model you trained before. Use the procedure to flip HOG to generate `w_flipped`. Then visualize both `w` and `w_flipped` as done in [Sect. 1.3](#sect13). Convince yourself that flipping was successful.\n",
    "\n",
    "We have now two models, `w` and `w_flipped`, one for each view of the object.\n",
    "\n",
    "> **Task:** Run both models in turn on the same image, obtaining two list of bounding boxes. Find a way to merge the two lists and visualise the top detections. Convince yourself that you can now detect objects facing either way.\n",
    "\n",
    "**Hint:** Recall how redundant detections can be removed using non-maximum suppression.\n",
    "\n",
    "**Congratulations: This concludes the practical!**\n",
    "\n",
    "[^nn]: This is part of the MatConvNet toolbox for convolutional neural networks. Nevertheless, there is no neural network discussed here.\n",
    "\n",
    "[1]: images/cover.jpeg \"cover.jpeg\"\n",
    "\n",
    "## History\n",
    "\n",
    "* Used in the Oxford AIMS CDT, 2014-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
