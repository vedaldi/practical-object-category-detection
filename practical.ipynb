{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object category detection practical\n",
    "\n",
    "This is an [Oxford Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg) computer vision practical, authored by [Andrea Vedaldi](http://www.robots.ox.ac.uk/~vedaldi/) and Andrew Zisserman (Release 2018a).\n",
    "\n",
    "![cover](data/cover.jpg)\n",
    "\n",
    "The goal of *object category detection* is to identify and localize objects of a given type in an image. Examples applications include detecting pedestrian, cars, or traffic signs in street scenes, objects of interest such as tools or animals in web images, or particular features in medical image.\n",
    "\n",
    "Given an object type, such as *people*, a *detector* receives as input an image and produces as output zero, one, or more bounding boxes around each occurrence of the object in the image. The key challenge is that the detector needs to find objects regardless of their location and scale in the image, as well as pose and other variation factors, such as clothing, illumination, occlusions, etc.\n",
    "\n",
    "This practical explores basic techniques in visual object detection, focusing on *image based models*. The appearance of image patches containing objects is learned using statistical analysis. Then, in order to detect objects in an image, the statistical model is applied to image windows extracted at all possible scales and locations, in order to identify which ones, if any, contain the object.\n",
    "\n",
    "In more detail, the practical explores the following topics: (i) using HOG features to describe image regions, (ii) building a HOG-based sliding-window detector to localize objects in images; (iii) working with multiple scales and multiple object occurrences; (iv) using a linear support vector machine to learn the appearance of objects; (v) evaluating an object detector in term of average precision; (vi) learning an object detector using hard negative mining.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bw}{\\mathbf{w}}\n",
    "\\newcommand{\\bp}{\\mathbf{p}}\n",
    "\\newcommand{\\cP}{\\mathcal{P}}\n",
    "\\newcommand{\\cN}{\\mathcal{N}}\n",
    "\\newcommand{\\vv}{\\operatorname{vec}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## Part 1: Detection fundamentals\n",
    "\n",
    "As running example, we consider the problem of street sign detection, using the data from the [German Traffic Sign Detection Benchmark](http://benchmark.ini.rub.de/?section=gtsdb&subsection=news). This data consists of a number of example traffic images, as well as a number of larger test images containing one or more traffic signs at different sizes and locations. It also comes with *ground truth* annotation, i.e. with specified bounding boxes and sign labels for each sign occurrence, which is required to evaluate the quality of the detector.\n",
    "\n",
    "In this part we will build a basic sliding-window object detector based on HOG features. Follow the steps below.\n",
    "\n",
    "### Step 1.0: Loading the training data\n",
    "\n",
    "First, we will load a Python data structure containing the data for the practical. To this end, we use the `lab.load_data` function of the `lab` package and store the result in a dictionary `imdb`. This dictionary has fields:\n",
    "\n",
    "* `imdb['train']['images']`: a list of train image names.\n",
    "* `imdb['train']['boxes']`: a $N\\times 4$ tensor of object bounding boxes, in the form $[x_\\text{min},y_\\text{min},x_\\text{max},y_\\text{max}]$.\n",
    "* `imdb['train']['box_images']`: for each bounding box, the name of the image containing it.\n",
    "* `imdb['train']['box_labels']`: for each bounding box, the object label.\n",
    "* `imdb['train']['box_patches']`: a $N \\times 3 64 \\times 64$ tensor of image patches, one for each training object. Patches are in RGB format.\n",
    "\n",
    "An analogous set of fields `imdb['val']` are provided for the validation data. Familiarise yourself with the contents of these variables.\n",
    "\n",
    "> **Task:** Run the following code to load the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "import lab\n",
    "import torch\n",
    "import copy\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "imdb = lab.load_data('mandatory') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** why is there a `imdb['train']['images']` and a `imdb['train']['box_images']` fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Visualize the training image patches\n",
    "\n",
    "Next, we use the functon `lab.imarraysc` to display the image patches stored in `imdb['train']['box_patches']`. The `lab.imarraysc` takes as input a $N\\times3\\times H\\times W$ tensor of $N$ RGB images of size $H\\times W$.\n",
    "\n",
    "> **Task:** Run the following code to visualize the training image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the training patches.\n",
    "plt.figure(1, figsize=(12,12))\n",
    "lab.imarraysc(imdb['train']['box_patches'])\n",
    "plt.title('Training patches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the mean of these patches using the function `imsc`, which instead takes as input a single $3\\times H\\times W$ image.\n",
    "\n",
    "> **Task:** Run the following code to visualize the mean training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average patch.\n",
    "plt.figure(2,figsize=(8,8))\n",
    "lab.imsc(imdb['train']['box_patches'].mean(0))\n",
    "plt.title('Training patches mean') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** what can you deduce about the object variability from the average image?\n",
    "\n",
    "> **Question:** most boxes extend slightly around the object extent. Why do you think this may be valuable in learning a detector?\n",
    "\n",
    "### Step 1.2: Extract HOG features from the training patches\n",
    "\n",
    "Object detectors usually work on top of a layer of low-level features. We will use a *Convolutional Neural Network* to extract such features. However, instead of learning it from data as it would be customary done, we take a shortcut and create an *handcrafted* network that computes the HOG (*Histogram of Oriented Gradients*) features. In this manner, we can still make efficient use of the underlying PyTorch library.\n",
    "\n",
    "In order to learn a model of the object, we start by extracting features from the image patches corresponding to the available training examples. HOG is computed by the `lab.HOGNet()` neural network. This network takes as input a grayscale or RGB image (or a batch of $N$ images), represented as a PyTorch tensor $N \\times C \\times W \\times H$ array where $C$ is either 1 or 3. The output is a $N \\times 27 \\times W/\\text{cell-size} \\times H/\\text{cell-size}$ dimensional array, where `cell_size` is 8 by default.\n",
    "\n",
    "> **Task:** Run the following code to extract the HOG representation of the training patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HOG extractor neural network.\n",
    "hog_extractor = lab.HOGNet()\n",
    "\n",
    "# Get the HOG representation of the trianing patches.\n",
    "hog_train = hog_extractor(imdb['train']['box_patches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1.3\"></a>\n",
    "\n",
    "### Step 1.3: Learn a simple HOG template model \n",
    "\n",
    "A very basic object model can be obtained by averaging the features of the example objects, then subtracting the mean feature value, as follows.\n",
    "\n",
    "> **Task:** Run the following colde to train a basic model for a target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the HOG of the corresponding training images\n",
    "w = torch.mean(hog_train, 0, keepdim=True)\n",
    "w = w - w.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be visualized by *rendering* `w` as if it was a HOG feature array. This can be done using the `hog_extractor.to_image` method. This method takes a $N\\times 27 \\times H\\times W$ HOG tensor and return a corresponding $N\\times 1\\times H_r\\times W_r$ tensor of rendered HOG images.\n",
    "\n",
    "> **Task:** Run the following code to visualize the model `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the HOG descriptor\n",
    "wim = hog_extractor.to_image(w)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6,6))\n",
    "lab.imsc(wim[0]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time to study this plot and make sure you understand what is visualized.\n",
    "\n",
    "> **Question:** Can you make sense of the resulting plot?\n",
    "\n",
    "### Step 1.4: Apply the model to a test image\n",
    "\n",
    "The model is matched to a test image by: (i) extracting the HOG features of the image and (ii) convolving the model over the resulting feature map. This can be conveniently achieved by using PyTorch convolution operator `nn.Conv2d` by wrapping the model in a corresponding convolutional layer.\n",
    "\n",
    "> **Task:** Run the following code to apply the learned model to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Wrap the model w in a convolutional layer.\n",
    "model = nn.Conv2d(27, 1, w.shape[2:], bias=False)\n",
    "model.weight.data = w\n",
    "print(\"Model parameters:\", model)\n",
    "\n",
    "# Extract the HOG representation of a test image.\n",
    "im = lab.imread('data/mandatory.jpg')\n",
    "hog = hog_extractor(im)\n",
    "\n",
    "# Apply the model.\n",
    "scores = model(hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tasks:** \n",
    "> 1. Work out the dimension of the `scores` arrays. Then, check your result with the dimension of the array computed by MATLAB.\n",
    "> 2. Visualize the image `im` and the `scores` array using the provided example code below. Does the result match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image.\n",
    "plt.figure(1, figsize=(8,8))\n",
    "lab.imsc(im[0])\n",
    "plt.title('Input image')\n",
    "\n",
    "# Visualize its HOG representation.\n",
    "plt.figure(2, figsize=(8,8))\n",
    "lab.imsc(hog_extractor.to_image(hog)[0])\n",
    "plt.title('HOG representation')\n",
    "\n",
    "# Visualize the scores as a heatmap.\n",
    "plt.figure(3, figsize=(8,8))\n",
    "lab.imsc(scores[0])\n",
    "plt.title('Scores') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Extract the top detection\n",
    "\n",
    "Now that the model has been applied to the image, we have a response map `scores`. To extract a detection from this, we (i) find which image boxes correspond to each entry in the `scores` tensor and (ii) find the box that has mximum score.\n",
    "\n",
    "The first step is done by the `lab.boxes_for_scores` function, which takes as input a $1\\times H\\times W$ score map and returns as output a $4\\times H\\times W$ tensor `boxes` where `boxes[:,v_,u_]` is a vector $[u_0,v_0,u_1,v_1]$ representing the bounding box in image space corresponding to score value `scores[1,v_,u_]`. Here $(u_0,v_0)$ is the upper-left corner of the box and $(u_1,v_1)$ the lower-right.\n",
    "\n",
    "The second step is done by rearranging `boxes` as a $N \\times 4$ tensor representing a list of $N=HW$ boxes, one per row, rearranging `scores` as a `N` vector, finding the maximum element of the latter, and use the corresponding index to pick a box from `boxes`.\n",
    "\n",
    "> **Question:** Inspect the code below. Why `cell_size` is involved in the calculations performed by `boxes_for_scores`?\n",
    "\n",
    "> **Task.** Run the following code to find the maximally-scored box and plot a corresponding bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the box coordinates for each score map location as a 4 x H x W tensor where\n",
    "# score[0] is a 1 x H x W tensor.\n",
    "boxes = lab.boxes_for_scores(model, scores[0])\n",
    "\n",
    "# Convert `boxes` into a N x 4 list.\n",
    "boxes = boxes.reshape(4,-1).permute(1,0)\n",
    "\n",
    "# Do the same for `scores` and pick the maximum.\n",
    "best, best_index = torch.max(scores[0].reshape(-1), 0)\n",
    "box = boxes[best_index]\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure(1, figsize=(10,10))\n",
    "lab.imsc(im[0])\n",
    "lab.plot_box(box)\n",
    "plt.title(f'Detected object (score: {best:.3f})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Use the example code to plot the image and overlay the bounding box of the detected object. Did it work as expected?\n",
    "\n",
    "> **[Optional] How the score map is converted into boxes.** The code of `lab.boxes_to_scores` can be inspected [here](lab.py).\n",
    "> The maximum is found in two steps, by first maximizing scores along dimension 2 (height) and then dimension 1 (width), obtaining indices `u` and `v`.\n",
    ">\n",
    "> `u` and `v` are in units of HOG cells. We convert this into pixel coordinates by multiplying by the HOG `cell_size`, which is also the downsampling factor applied by the `HOGNet` feature extractor.\n",
    ">\n",
    "> The size of the model template in number of HOG cell can be found as the size of the kernel in `model` (note that the kernel is square in this case, so PyTorch only saves one dimension). This is then converted in pixels by multipling by `cell_size` as before.\n",
    ">\n",
    "> In this manner, we can deduce the coordiantes of the upper-left and bottom-right conrenrs of the object bounding box and plot it.\n",
    ">\n",
    "> **Note:** the bounding box encloses exactly all the pixel of the HOG template. In MATLAB, pixel centers have integer coordinates and pixel borders are at a distance $\\pm1/2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple scales and learning with an SVM\n",
    "\n",
    "In this second part, we will: (i) extend the detector to search objects at multiple scales and (ii) learn a better model using a support vector machine. Let's start by loading a subset of the data targeting a particular class of road signs.\n",
    "\n",
    "> **Task:** Run the following code to reload the data structure keeping only the \"mandatory\" signs. The \"mandatory\" class is simply the union of all mandatory traffic signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = lab.load_data(meta_class='mandatory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Multi-scale detection\n",
    "\n",
    "First, we demonstrate that our vanilla detector lacks scale invariance: if the image is presented at different resoltuons, then different results are obtained.\n",
    "\n",
    "> **Task:** Run the following code to perform *singe-scale detection* on three different versions of the same image, differing by size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for t, s in enumerate([1.5, 1, .6]):\n",
    "    # Load a target image in PIL format.\n",
    "    image = Image.open('data/mandatory.jpg')\n",
    "    \n",
    "    # Change the resolution of the image.\n",
    "    image = image.resize([int(s*x) for x in image.size], Image.LANCZOS)\n",
    "\n",
    "    # Run the detector as shown above.\n",
    "    model = nn.Conv2d(27, 1, w.shape[2:], bias=False)\n",
    "    model.weight.data = w\n",
    "    hog = hog_extractor(lab.pil_to_torch(image))\n",
    "    scores = model(hog)\n",
    "    boxes = lab.boxes_for_scores(model, scores[0])\n",
    "    boxes = boxes.reshape(4,-1).permute(1,0)\n",
    "    best, best_index = torch.max(scores[0].reshape(-1), 0)\n",
    "    box = boxes[best_index]\n",
    "\n",
    "    # Plot the results for this resolution.\n",
    "    plt.figure(t, figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    lab.plot_box(box)\n",
    "    plt.title(f'Detected object (score: {best:.3f})') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question.** Does the result look right?\n",
    "\n",
    "This is a practical issue as objects exist in images at sizes different from one of the learned template. In order to find objects of all sizes, we scale the image up and down and search for the object over and over again.\n",
    "\n",
    "We solve this issue by running **detection at multiple scales**. The set of searched scales $s_0,s_1,\\dots$ is defined logarithmically as $s_i = 2^{\\frac{i}{S} - o}$ where $S$ is the number of octave subdivisions and $o$ the minimum octave.\n",
    "\n",
    "> **Task**: Run the following code to define the set of searched scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scale space configuraiton\n",
    "min_octave = -1\n",
    "max_octave = 3\n",
    "num_octave_subdivisions = 3\n",
    "scales = 2**np.linspace(min_octave, max_octave, num_octave_subdivisions * (max_octave - min_octave + 1))\n",
    "\n",
    "# Print the scale values\n",
    "print('Scales: ', ', '.join([f\"{x:.2f}\" for x in scales]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab code provides a member function `HOGNet.detect_at_multiple_scales` that implements multi-scale detection using these scales.\n",
    "\n",
    "> **Question:** [Open](lab.py) and study the `HOGNet.detect_at_multiple_scales` function. Convince yourself that it is the same code as before, but operated after rescaling the image a number of times. \n",
    "Next, we use the function `hog_extractor.detect_at_multiple_scales` to perform multi-scale detection.\n",
    "\n",
    "The code below applies multi-scale detection to the three image sizes tested above.\n",
    "\n",
    "> **Task:** Run the following code to perform *multi-scale detection* on three different versions of the same image, differing by size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, s in enumerate([1.5, 1, .6]):\n",
    "    # Load a target image in PIL format.\n",
    "    image = Image.open('data/mandatory.jpg')\n",
    "    \n",
    "    # Change the resolution of the image.\n",
    "    image = image.resize([int(s*x) for x in image.size], Image.LANCZOS)\n",
    "\n",
    "     # Get all boxes and scores\n",
    "    boxes, scores, _ = hog_extractor.detect_at_multiple_scales(w, scales, image)\n",
    "\n",
    "    # Pick the best box\n",
    "    best, best_index = torch.max(scores, 0)\n",
    "    box = boxes[best_index]\n",
    "\n",
    "    # Plot the results for this resolution.\n",
    "    plt.figure(t, figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    lab.plot_box(box)\n",
    "    plt.title(f'Detected object (score: {best:.3f})') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Compare this result with the one obtained above. What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Collect positive and negative training data\n",
    "\n",
    "The model learned so far is too weak to work well. It is now time to use an SVM to learn a better one. In order to do so, we need to prepare suitable data. We already have positive examples (features extracted from object patches).\n",
    "\n",
    "In order to collect negative examples (features extracted from non-object patches), we loop through a number of training images and sample patches uniformly. For speed, we only scan a subset of the images.\n",
    "\n",
    "> **Task:** Run the following code to extract positive and negative HOG vectors.\n",
    "\n",
    "> **Question:** How many negative examples are we collecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect positive training data\n",
    "pos = hog_train\n",
    "\n",
    "# Collect negative training data\n",
    "neg = []\n",
    "num_training_images = 5\n",
    "for image_path in imdb['train']['images'][:num_training_images]:\n",
    "    print(f\"Processing image {image_path}\")\n",
    "    image = Image.open(image_path)    \n",
    "    hog = hog_extractor(lab.pil_to_torch(image))    \n",
    "    patches = nn.Unfold(w.shape[2:], stride=5)(hog)\n",
    "    n = patches.shape[1]\n",
    "    patches = patches[:, :, :-1:n//100].permute(0,2,1)\n",
    "    patches = patches.reshape(patches.shape[1], *w.shape[1:])\n",
    "    neg.append(patches)\n",
    "    \n",
    "neg = torch.cat(neg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Learn a model with an SVM\n",
    "\n",
    "Now that we have the data, we can learn an SVM model. To this end we will use the `lab.svm_scda()` function. This function requires the data to be in a $D \\times N$ matrix, where $D$ are the feature dimensions and $N$ the number of training points. We also need a vector of binary labels, +1 for positive points and -1 for negative ones:\n",
    "\n",
    "> **Task:** Run the folloing code to prepare the data for the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the data into a matrix with one datum per row\n",
    "x = torch.cat((pos, neg), 0).reshape(len(pos) + len(neg), -1)\n",
    "\n",
    "# Obtain the corresponding label vector\n",
    "y = torch.cat((torch.ones(len(pos)), -torch.ones(len(neg))), 0)\n",
    "\n",
    "print(f\"The shape of x is {list(x.shape)}\")\n",
    "print(f\"The shape of y is {list(y.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to set the parameter $\\lambda$ of the SVM solver. For reasons that will become clearer later, we use instead the equivalent $C$ parameter. Learning the SVM is then a one-liner.\n",
    "\n",
    "> **Task.** Run the following code to train the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the SVM parameter.\n",
    "C = 10\n",
    "lam = 1 / (C * (len(pos) + len(neg)))\n",
    "\n",
    "# Run the SVM.\n",
    "w, b = lab.svm_sdca(x, y, lam=lam)\n",
    "w = w.reshape(1, *pos[0].shape)\n",
    "\n",
    "# Plot the learned model\n",
    "wim = hog_extractor.to_image(w)\n",
    "plt.figure(figsize=(6, 6))\n",
    "lab.imsc(wim[0]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Visualize the learned model `w` using the supplied code. Does it differ from the naive model learned before? How?\n",
    "\n",
    "### Step 2.4: Evaluate the learned model\n",
    "\n",
    "> **Task:** Use the `detect_at_multiple_scale` seen above to evaluate the new SVM-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, s in enumerate([1, .75, .5]):\n",
    "    # Load a target image in PIL format.\n",
    "    image = Image.open('data/mandatory.jpg')\n",
    "    \n",
    "    # Change the resolution of the image.\n",
    "    image = image.resize([int(s*x) for x in image.size], Image.LANCZOS)\n",
    "\n",
    "    # Get all boxes and scores\n",
    "    boxes, scores, _ = hog_extractor.detect_at_multiple_scales(w, scales, image)\n",
    "\n",
    "    # Pick the best box\n",
    "    best, best_index = torch.max(scores, 0)\n",
    "    box = boxes[best_index]\n",
    "\n",
    "    # Plot the results.\n",
    "    plt.figure(t, figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    lab.plot_box(box)\n",
    "    plt.title(f'Detected object (score: {best:.3f})') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Does the learned model perform better than the naive average?\n",
    "\n",
    "> **Task:** Try different images. Does this detector work all the times? If not, what types of mistakes do you see? Are these mistakes reasonable?\n",
    "\n",
    "<a id=\"part3\"></a>\n",
    "## Part 3: Multiple objects and evaluation\n",
    "\n",
    "### Step 3.1: Multiple detections\n",
    "\n",
    "Detecting at multiple scales is insufficient: we must also allow for more than one object occurrence in the image. In order to to so, rather than looking for the best box in the lot, we select several using *non-maximum suppression* (NMS).\n",
    "\n",
    "The algorithm is simple: start from the highest-scoring detection, then remove any other detection whose overlap is greater than a threshold. The overlap metric used to compare a candidate detection to a ground truth bounding box is defined as the *ratio of the area of the intersection over the area of the union* of the two bounding boxes:\n",
    "$$\n",
    "\\operatorname{overlap}(A,B) = \\frac{|A\\cap B|}{|A \\cup B|}.\n",
    "$$\n",
    "\n",
    "This algorithm is implemented by the function `nms()` below, which returns a boolean vector `retain` of detections to preserve.\n",
    "\n",
    "> **Tasks:**\n",
    "> 1. Study the `lab.nms()` [function](lab.py) and make sure you understand how it works.\n",
    "> 2. Study the `lab.topn()` [function](lab.py) and make sure you understand how it works.\n",
    "> 3. Run the following code to obtain the top non-maxima supporessed boxes.\n",
    "\n",
    "> **Question:** After non-maxima suppression we still get a significant number of boxes. Why do we want to return so many responses? In practice, it is unlikely that more than a handful of object occurrences may be contained in any given image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a target image in PIL format.\n",
    "image = Image.open('data/signs-sample-image.jpg')\n",
    "\n",
    "# Get all boxes and scores.\n",
    "boxes, scores, _ = hog_extractor.detect_at_multiple_scales(w, scales, image)\n",
    "\n",
    "# Get the top boxes.\n",
    "boxes, scores, _ = lab.topn(boxes, scores, 100)\n",
    "\n",
    "# Apply non-maxima suppression.\n",
    "boxes, scores, _ = lab.nms(boxes, scores)\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure(t, figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "for box in boxes:\n",
    "    lab.plot_box(box)\n",
    "plt.title(f'Showing {len(boxes)} boxs') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Detector evaluation\n",
    "\n",
    "We are now going to look at properly evaluating our detector. We use the [PASCAL VOC criterion](http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/devkit_doc.pdf), computing *Average Precision (AP)*. Consider a test image containing a number of ground truth object occurrences $(g_1,\\dots,g_m)$ and a list $(b_1,s_1),\\dots,(b_n,s_n)$ of candidate detections $b_i$ with score $s_i$. The following algorithm converts this data into a list of labels and scores $(s_i,y_i)$ that can be used to compute a precision-recall curve, for example using the `lab.pr()` function. The algorithm, implemented by `lab.eval_detections()`, is as follows:\n",
    "\n",
    "0. Prerequisite: Assume that the candidate detections $(b_i,s_i)$ are sorted by decreasing score $s_i$.\n",
    "1. Assign each candidate detection $(b_i,s_i)$ a true or false label $y_i \\in \\{+1,-1\\}$. To do so, for each candidate detection in order:\n",
    "    1. If there is a matching ground truth detection $g_j$ ($\\operatorname{overlap}(b_i,g_j)$ larger than 50%), the candidate detection is considered positive ($y_i=+1$). Furthermore, the ground truth detection is *removed from the list* and not considered further.\n",
    "    2. Otherwise, the candidate detection is negative ($y_i=-1$).\n",
    "2. Add each ground truth object $g_i$ that is still unassigned to the list of candidates as pair $(g_j, -\\infty)$ with label $y_j=+1$.\n",
    "\n",
    "> **Questions:**\n",
    "> * Why are ground truth detections removed after being matched?\n",
    "> * What happens if an object is detected twice?\n",
    "> * Can you explain why unassigned ground-truth objects are added to the list of candidates with $-\\infty$ score?\n",
    "\n",
    "In order to apply this algorithm, we first need to find the ground truth bounding boxes in the test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first validation image.\n",
    "image = imdb['val']['images'][1]\n",
    "pil_image = Image.open(image)\n",
    "\n",
    "# Pick all the gt boxes in the selected image.\n",
    "sel = [i for i, box_image in enumerate(imdb['val']['box_images']) if box_image == image]\n",
    "gt_boxes = imdb['val']['boxes'][sel]\n",
    "\n",
    "# Plot the images and its boxes.\n",
    "plt.figure(1, figsize=(8,8))\n",
    "plt.imshow(pil_image)\n",
    "for box in gt_boxes:\n",
    "    lab.plot_box(box, color='y')\n",
    "plt.title(f'Found {len(gt_boxes)} boxes in image {image}.') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the detector code as before to extract the signs.\n",
    "\n",
    "> **Task:** Use the code below to evaluate the detector on one image. Look carefully at the output and convince yourself that it makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detector\n",
    "boxes, scores, _ = hog_extractor.detect_at_multiple_scales(w, scales, pil_image)\n",
    "boxes, scores, _ = lab.topn(boxes, scores, 100)\n",
    "boxes, scores, _ = lab.nms(boxes, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can evaluate the detections by matchign them to the ground truth.\n",
    "\n",
    "> **Task:** Use the code below to compare the detections to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the detector and plot the results.\n",
    "plt.figure(1, figsize=(12,12))\n",
    "plt.imshow(pil_image)\n",
    "results = lab.eval_detections(gt_boxes, boxes, plot=True)\n",
    "plt.title('yellow: gt, red: false detections, green: ture detections') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to plot the PR curve and compute AP.\n",
    "\n",
    "> **Task:** Use the code below to compute and plot the PR curve.\n",
    "\n",
    "> **Question:** There are a large number of errors in each image. Should you worry?  In what manner is the PR curve affected? How would you eliminate the vast majority of those in a practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "_, _, ap = lab.pr(results['labels'], scores, misses=results['misses'])\n",
    "plt.title(f'Average precision: {ap*100:.2f}%') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Evaluation on multiple images\n",
    "\n",
    "Evaluation is typically done on multiple images rather than just one. This is implemented by the `lab.evaluate_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the first 20 validation images.\n",
    "results = lab.evaluate_model(imdb, hog_extractor, w, scales=scales, subset=('val',0,20))\n",
    "\n",
    "# PLot the resulting AP.\n",
    "plt.figure(1, figsize=(5,5))\n",
    "_, _, ap = lab.pr(results['labels'], results['scores'], misses=results['misses'])\n",
    "plt.title(f'Average precision: {ap*100:.2f}%') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** [Open](lab.py) `lab.evaluate_model` and make sure you understand the main steps of the evaluation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Hard negative mining\n",
    "\n",
    "This part explores more advanced learning methods. So far, the SVM has been learned using a small and randomly sampled number of negative examples. However, in principle, every single patch that does not contain the object can be considered as a negative sample. These are of course too many to be used in practice; unfortunately, random sampling is ineffective as the most interesting (confusing) negative samples are a very small and special subset of all the possible ones.\n",
    "\n",
    "*Hard negative mining* is a simple technique that allows finding a small set of key negative examples. The idea is simple: we start by initializing the model randomly, and then we alternate between evaluating the model on the training data to find erroneous responses and adding the corresponding examples to the training set.\n",
    "\n",
    "### Step 4.1: Train with hard negative mining\n",
    "\n",
    "The function `collect_hard_negatives` is called by the previously-defined function `evaluate_model` after the detector is run on each image to extract the HOG descriptors of the hardest negative patches. These patches are the ones that are highly scored by the models *and* incorrect according to the evaluation procedure. Recall that the result of the evaluation is a label which is +1 for good detections and -1 for incorrect ones. Collectively, these labels are stored in a $N$ tensor `labels` for a $N\\times 4$ tensor `boxes` of detections.\n",
    "\n",
    "`boxes` is expressed in pixels, whereas we need to obtain the corresponding HOG descriptors. In order to do so, each box coordinates are revese-mapped to a certain scale pyramid level, and then the corresponding HOG patch is extracted.\n",
    "\n",
    "> **Task:** [Open](lab.py) the `collect_hard_negatives` function and inspect the code and make sure you understand how it works.\n",
    "\n",
    "Next, we repeat SVM training, as seen above, a number of times, progressively increasing the size of the `neg` array containing the negative samples. This is updated using the output of `evaluate_model`, that in turns calls `collect_hard_negatives` discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Collect positive training data.\n",
    "pos = hog_train\n",
    "\n",
    "def train_model(pos, imdb):\n",
    "    # Initialize the model randomly.\n",
    "    w = torch.randn((1, *pos[0].shape), dtype=torch.float32)\n",
    "\n",
    "    n = len(imdb['train']['images'])\n",
    "    for t in range(0, n, batch_size):\n",
    "        tp = min(t + batch_size, n)\n",
    "        print(f'Iteration {t}: running the model on images {t}-{tp-1}') ;\n",
    "\n",
    "        # Evaluate the model on the training data.\n",
    "        results = lab.evaluate_model(imdb, hog_extractor, w, scales=scales, subset=('train',t,tp), collect_negatives=True)\n",
    "\n",
    "        # Collect more negative training patches.\n",
    "        if t == 0:\n",
    "            neg = torch.cat(results['negatives'], 0)\n",
    "        else:\n",
    "            neg = torch.cat((neg, *results['negatives']), 0)\n",
    "        print(f\"Iteration {t}: collected {len(pos)} positive and {len(neg)} negative patches\")\n",
    "\n",
    "        # Learn the SVM.\n",
    "        C = 10\n",
    "        lam = 1 / (C * (len(pos) + len(neg)))\n",
    "        x = torch.cat((pos, neg), 0).reshape(len(pos) + len(neg), -1)\n",
    "        y = torch.cat((torch.ones(len(pos)), -torch.ones(len(neg))), 0)\n",
    "\n",
    "        plt.figure(3*t+1, figsize=(5, 5))\n",
    "        w, b = lab.svm_sdca(x, y, lam=lam)\n",
    "        w = w.reshape(1, *pos[0].shape)\n",
    "\n",
    "        # Plot the learned model.\n",
    "        wim = hog_extractor.to_image(w)\n",
    "        plt.figure(3*t+2, figsize=(5, 5))\n",
    "        lab.imsc(wim[0])\n",
    "    return w\n",
    "\n",
    "w = train_model(pos, imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Evaluate the model trained using hard negative mining\n",
    "\n",
    "> **Task:** Use the following code to evaluate AP on the first 20 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the first 20 validation images.\n",
    "results = lab.evaluate_model(imdb, hog_extractor, w, scales=scales, subset=('val',0,20))\n",
    "\n",
    "plt.figure(1, figsize=(6,6))\n",
    "_, _, ap = lab.pr(results['labels'], results['scores'], misses=results['misses'])\n",
    "plt.title(f'Average precision: {ap*100:.2f}%') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Use the following code to run the model on a validation image and visualzie the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first validation image.\n",
    "image = imdb['val']['images'][8]\n",
    "pil_image = Image.open(image)\n",
    "\n",
    "# Run the detector.\n",
    "boxes, scores, hogs = hog_extractor.detect_at_multiple_scales(w, scales, pil_image)\n",
    "boxes, scores, _ = lab.topn(boxes, scores, 100)\n",
    "boxes, scores, _ = lab.nms(boxes, scores)\n",
    "\n",
    "# Evaluate the detector and plot the results.\n",
    "sel = [i for i, box_image in enumerate(imdb['val']['box_images']) if box_image == image]\n",
    "gt_boxes = imdb['val']['boxes'][sel]\n",
    "plt.figure(1, figsize=(15,15))\n",
    "plt.imshow(pil_image)\n",
    "lab.eval_detections(gt_boxes, boxes, plot=True)\n",
    "plt.title('yellow: gt, green: true detections, red: false detections') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Train your own object detector\n",
    "\n",
    "**Skip on fast track**\n",
    "\n",
    "In this last part, you will learn your own object detector. To this end, open and look at `exercise5.m`. You will need to prepare the following data:\n",
    "\n",
    "### Step 5.1: Preparing the training data\n",
    "\n",
    "* A folder `data/pos` containing files `image1.jpg`, `image2.jpg`, ..., each containing a single cropped occurence of the target object. These crops can be of any size, but should be roughly square.\n",
    "* A folder `data/neg` containing images `image1.jpg`, `image2.jpg`, ..., that *do not* contain the target object at all.\n",
    "* A test image `data/test.jpg` containing the target object. This should not be one of the training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Understand the limitations of this simple detector and choose a target object that has a good chance of being learnable. \n",
    "\n",
    "**Hint:** Note in particular that object instances must be similar and roughly aligned. If your object is not symmetric, consider choosing instances that face a particular direction (e.g. left-facing horse head).\n",
    "\n",
    "### Step 5.2: Learn the model\n",
    "\n",
    "> **Task:** Use the code belwo to learn the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_data(hog_extractor):\n",
    "        from glob import glob\n",
    "        pos = []\n",
    "        for f in glob('data/pos/*.jpg'):\n",
    "            pil_image = Image.open(f)           \n",
    "            scaled_image = pil_image.resize([64, 64])\n",
    "            pos.append(lab.pil_to_torch(scaled_image))\n",
    "        pos = hog_extractor(torch.cat(pos, 0))\n",
    "            \n",
    "        imdb = {\n",
    "            'images': glob('data/neg/*.jpg'),\n",
    "            'boxes' : torch.Tensor(0,4),\n",
    "            'box_images' : [],\n",
    "            'box_labels' : torch.Tensor(0,1),\n",
    "            'box_patches' : []\n",
    "        }\n",
    "        \n",
    "        return {'train': imdb}, pos\n",
    "\n",
    "imdb, pos = load_custom_data(hog_extractor)\n",
    "w = train_model(pos, imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(0,4).reshape(2,-1,4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3: Test the model\n",
    "\n",
    "Use the code supplied in `example5.m` to evaluate the SVM model on a test image and visualize the result as in [Stage 2.1](#stage2.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Make sure you get sensible results. Go back to step 5.1 if needed and adjust your data.\n",
    "\n",
    "**Hint:** For debugging purposes, try using one of your training images as test. Does it work at least in this case?\n",
    "\n",
    "### Step 5.4: Detecting symmetric objects with multiple aspects\n",
    "\n",
    "The basic detectors you have learned so far are *not* invariant to effects such as object deformations, out-of-plane rotations, and partial occlusions that affect most natural objects. Handling these effects requires additional sophistications, including using deformable templates, and a mixture of multiple templates.\n",
    "\n",
    "In particular, many objects in nature are symmetric and, as such, their images appear flipped when the objects are seen from the left or the right direction (consider for example a face). This can be handled by a pair of symmetric HOG templates. In this part we will explore this option.\n",
    "\n",
    "> **Task:** Using the procedure above, train a HOG template `w` for a symmetric object facing in one specific direction. For example, train a left-facing horse head detector.\n",
    "\n",
    "> **Task:** Collect test images containing the object facing in both directions. Run your detector and convince yourself that it works well only for the direction it was trained for.\n",
    "\n",
    "HOG features have a well defined structure that makes it possible to predict how the features transform when the underlying image is flipped. The transformation is in fact a simple *permutation* of the HOG elements. For a given spatial cell, HOG has 31 dimensions. The following code permutes the dimension to flip the cell around the vertical axis:\n",
    "\n",
    "    perm = vl_hog('permutation') ;\n",
    "    hog_flipped = hog(perm) ;\n",
    "\n",
    "Note that this permutation applies to a *single* HOG cell. However, the template is a $H \\times W \\times 31$ dimensional array of HOG cells.\n",
    "\n",
    "> **Task:** Given a `hog` array of dimension $H \\times W \\times 31$, write MATLAB code to obtain the flipped feature array `hog_flipped`.\n",
    "\n",
    "**Hint:** Recall that the first dimension spans the vertical axis, the second dimension the horizontal axis, and the third dimension feature channels. `perm` should be applied to the last dimension. Do you need to permute anything else?\n",
    "\n",
    "Now let us apply flipping to the model trained earlier:\n",
    "\n",
    "> **Task:** Let `w` be the model you trained before. Use the procedure to flip HOG to generate `w_flipped`. Then visualize both `w` and `w_flipped` as done in [Sect. 1.3](#sect13). Convince yourself that flipping was successful.\n",
    "\n",
    "We have now two models, `w` and `w_flipped`, one for each view of the object.\n",
    "\n",
    "> **Task:** Run both models in turn on the same image, obtaining two list of bounding boxes. Find a way to merge the two lists and visualise the top detections. Convince yourself that you can now detect objects facing either way.\n",
    "\n",
    "**Hint:** Recall how redundant detections can be removed using non-maximum suppression.\n",
    "\n",
    "**Congratulations: This concludes the practical!**\n",
    "\n",
    "## History\n",
    "\n",
    "* Used in the Oxford AIMS CDT, 2014-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
